{"cells":[{"cell_type":"markdown","source":"# March Madness Predictions","metadata":{"tags":[],"cell_id":"daeec90c0d8640aaa85adfeb7b94a8a5","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h1"}},{"cell_type":"markdown","source":"Jason Lott, Brendan LaPuma, Eamon Weingold","metadata":{"tags":[],"cell_id":"1647f6f124d84e50b36816419dba3b63","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"code","source":"import pandas as pd, numpy as np\nimport tensorflow as tf","metadata":{"cell_id":"d1a914b692c346d6ab7a786b6c4cd6aa","source_hash":"85e52fe4","execution_start":1678496020443,"execution_millis":5712,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stderr","text":"2023-03-11 00:53:40.422295: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-03-11 00:53:40.632976: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n2023-03-11 00:53:40.633014: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n2023-03-11 00:53:40.683536: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2023-03-11 00:53:42.746772: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n2023-03-11 00:53:42.746919: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n2023-03-11 00:53:42.746936: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"df = pd.read_csv('data/MRegularSeasonDetailedResults.csv')","metadata":{"cell_id":"c984fb65068f46abb443417f4b43d40b","source_hash":"427106e0","execution_start":1678496026152,"execution_millis":267,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":2},{"cell_type":"code","source":"winners = df.iloc[:, np.r_[0:4, 6:21]]\nlosers = df.iloc[:, np.r_[0, 1, 4:8, 21:34]]","metadata":{"cell_id":"4e9213febb91466a8ab5b783b35a63d0","source_hash":"46cce263","execution_start":1678496026434,"execution_millis":5,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":3},{"cell_type":"code","source":"winners","metadata":{"cell_id":"7767c7bbd6d84d13bae8cf472bf57d87","source_hash":"365c9589","execution_start":1678496026442,"execution_millis":80,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"application/vnd.deepnote.dataframe.v3+json":{"column_count":19,"row_count":106834,"columns":[{"name":"Season","dtype":"int64"},{"name":"DayNum","dtype":"int64"},{"name":"WTeamID","dtype":"int64"},{"name":"WScore","dtype":"int64"},{"name":"WLoc","dtype":"object"},{"name":"NumOT","dtype":"int64"},{"name":"WFGM","dtype":"int64"},{"name":"WFGA","dtype":"int64"},{"name":"WFGM3","dtype":"int64"},{"name":"WFGA3","dtype":"int64"},{"name":"WFTM","dtype":"int64"},{"name":"WFTA","dtype":"int64"},{"name":"WOR","dtype":"int64"},{"name":"WDR","dtype":"int64"},{"name":"WAst","dtype":"int64"},{"name":"WTO","dtype":"int64"},{"name":"WStl","dtype":"int64"},{"name":"WBlk","dtype":"int64"},{"name":"WPF","dtype":"int64"},{"name":"_deepnote_index_column","dtype":"int64"}],"rows":[{"Season":"2003","DayNum":"10","WTeamID":"1104","WScore":"68","WLoc":"N","NumOT":"0","WFGM":"27","WFGA":"58","WFGM3":"3","WFGA3":"14","WFTM":"11","WFTA":"18","WOR":"14","WDR":"24","WAst":"13","WTO":"23","WStl":"7","WBlk":"1","WPF":"22","_deepnote_index_column":"0"},{"Season":"2003","DayNum":"10","WTeamID":"1272","WScore":"70","WLoc":"N","NumOT":"0","WFGM":"26","WFGA":"62","WFGM3":"8","WFGA3":"20","WFTM":"10","WFTA":"19","WOR":"15","WDR":"28","WAst":"16","WTO":"13","WStl":"4","WBlk":"4","WPF":"18","_deepnote_index_column":"1"},{"Season":"2003","DayNum":"11","WTeamID":"1266","WScore":"73","WLoc":"N","NumOT":"0","WFGM":"24","WFGA":"58","WFGM3":"8","WFGA3":"18","WFTM":"17","WFTA":"29","WOR":"17","WDR":"26","WAst":"15","WTO":"10","WStl":"5","WBlk":"2","WPF":"25","_deepnote_index_column":"2"},{"Season":"2003","DayNum":"11","WTeamID":"1296","WScore":"56","WLoc":"N","NumOT":"0","WFGM":"18","WFGA":"38","WFGM3":"3","WFGA3":"9","WFTM":"17","WFTA":"31","WOR":"6","WDR":"19","WAst":"11","WTO":"12","WStl":"14","WBlk":"2","WPF":"18","_deepnote_index_column":"3"},{"Season":"2003","DayNum":"11","WTeamID":"1400","WScore":"77","WLoc":"N","NumOT":"0","WFGM":"30","WFGA":"61","WFGM3":"6","WFGA3":"14","WFTM":"11","WFTA":"13","WOR":"17","WDR":"22","WAst":"12","WTO":"14","WStl":"4","WBlk":"4","WPF":"20","_deepnote_index_column":"4"},{"Season":"2003","DayNum":"11","WTeamID":"1458","WScore":"81","WLoc":"H","NumOT":"0","WFGM":"26","WFGA":"57","WFGM3":"6","WFGA3":"12","WFTM":"23","WFTA":"27","WOR":"12","WDR":"24","WAst":"12","WTO":"9","WStl":"9","WBlk":"3","WPF":"18","_deepnote_index_column":"5"},{"Season":"2003","DayNum":"12","WTeamID":"1161","WScore":"80","WLoc":"H","NumOT":"0","WFGM":"23","WFGA":"55","WFGM3":"2","WFGA3":"8","WFTM":"32","WFTA":"39","WOR":"13","WDR":"18","WAst":"14","WTO":"17","WStl":"11","WBlk":"1","WPF":"25","_deepnote_index_column":"6"},{"Season":"2003","DayNum":"12","WTeamID":"1186","WScore":"75","WLoc":"N","NumOT":"0","WFGM":"28","WFGA":"62","WFGM3":"4","WFGA3":"14","WFTM":"15","WFTA":"21","WOR":"13","WDR":"35","WAst":"19","WTO":"19","WStl":"7","WBlk":"2","WPF":"21","_deepnote_index_column":"7"},{"Season":"2003","DayNum":"12","WTeamID":"1194","WScore":"71","WLoc":"N","NumOT":"0","WFGM":"28","WFGA":"58","WFGM3":"5","WFGA3":"11","WFTM":"10","WFTA":"18","WOR":"9","WDR":"22","WAst":"9","WTO":"17","WStl":"9","WBlk":"2","WPF":"23","_deepnote_index_column":"8"},{"Season":"2003","DayNum":"12","WTeamID":"1458","WScore":"84","WLoc":"H","NumOT":"0","WFGM":"32","WFGA":"67","WFGM3":"5","WFGA3":"17","WFTM":"15","WFTA":"19","WOR":"14","WDR":"22","WAst":"11","WTO":"6","WStl":"12","WBlk":"0","WPF":"13","_deepnote_index_column":"9"}]},"text/plain":"        Season  DayNum  WTeamID  WScore WLoc  NumOT  WFGM  WFGA  WFGM3  WFGA3  \\\n0         2003      10     1104      68    N      0    27    58      3     14   \n1         2003      10     1272      70    N      0    26    62      8     20   \n2         2003      11     1266      73    N      0    24    58      8     18   \n3         2003      11     1296      56    N      0    18    38      3      9   \n4         2003      11     1400      77    N      0    30    61      6     14   \n...        ...     ...      ...     ...  ...    ...   ...   ...    ...    ...   \n106829    2023     113     1403      74    A      0    29    47      7     12   \n106830    2023     113     1405      84    H      0    32    61     10     20   \n106831    2023     113     1429      65    A      0    21    51      7     27   \n106832    2023     113     1433      88    A      0    31    59     11     19   \n106833    2023     113     1437      64    A      0    25    56      6     19   \n\n        WFTM  WFTA  WOR  WDR  WAst  WTO  WStl  WBlk  WPF  \n0         11    18   14   24    13   23     7     1   22  \n1         10    19   15   28    16   13     4     4   18  \n2         17    29   17   26    15   10     5     2   25  \n3         17    31    6   19    11   12    14     2   18  \n4         11    13   17   22    12   14     4     4   20  \n...      ...   ...  ...  ...   ...  ...   ...   ...  ...  \n106829     9    14    5   31    11   16     4     1   17  \n106830    10    13    7   25    16    7     7     7   17  \n106831    16    22    7   27    11   12     7     3   15  \n106832    15    18    8   23    14    9     6     5   16  \n106833     8     9    3   21    13    7     7     1   16  \n\n[106834 rows x 19 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Season</th>\n      <th>DayNum</th>\n      <th>WTeamID</th>\n      <th>WScore</th>\n      <th>WLoc</th>\n      <th>NumOT</th>\n      <th>WFGM</th>\n      <th>WFGA</th>\n      <th>WFGM3</th>\n      <th>WFGA3</th>\n      <th>WFTM</th>\n      <th>WFTA</th>\n      <th>WOR</th>\n      <th>WDR</th>\n      <th>WAst</th>\n      <th>WTO</th>\n      <th>WStl</th>\n      <th>WBlk</th>\n      <th>WPF</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2003</td>\n      <td>10</td>\n      <td>1104</td>\n      <td>68</td>\n      <td>N</td>\n      <td>0</td>\n      <td>27</td>\n      <td>58</td>\n      <td>3</td>\n      <td>14</td>\n      <td>11</td>\n      <td>18</td>\n      <td>14</td>\n      <td>24</td>\n      <td>13</td>\n      <td>23</td>\n      <td>7</td>\n      <td>1</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2003</td>\n      <td>10</td>\n      <td>1272</td>\n      <td>70</td>\n      <td>N</td>\n      <td>0</td>\n      <td>26</td>\n      <td>62</td>\n      <td>8</td>\n      <td>20</td>\n      <td>10</td>\n      <td>19</td>\n      <td>15</td>\n      <td>28</td>\n      <td>16</td>\n      <td>13</td>\n      <td>4</td>\n      <td>4</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2003</td>\n      <td>11</td>\n      <td>1266</td>\n      <td>73</td>\n      <td>N</td>\n      <td>0</td>\n      <td>24</td>\n      <td>58</td>\n      <td>8</td>\n      <td>18</td>\n      <td>17</td>\n      <td>29</td>\n      <td>17</td>\n      <td>26</td>\n      <td>15</td>\n      <td>10</td>\n      <td>5</td>\n      <td>2</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2003</td>\n      <td>11</td>\n      <td>1296</td>\n      <td>56</td>\n      <td>N</td>\n      <td>0</td>\n      <td>18</td>\n      <td>38</td>\n      <td>3</td>\n      <td>9</td>\n      <td>17</td>\n      <td>31</td>\n      <td>6</td>\n      <td>19</td>\n      <td>11</td>\n      <td>12</td>\n      <td>14</td>\n      <td>2</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2003</td>\n      <td>11</td>\n      <td>1400</td>\n      <td>77</td>\n      <td>N</td>\n      <td>0</td>\n      <td>30</td>\n      <td>61</td>\n      <td>6</td>\n      <td>14</td>\n      <td>11</td>\n      <td>13</td>\n      <td>17</td>\n      <td>22</td>\n      <td>12</td>\n      <td>14</td>\n      <td>4</td>\n      <td>4</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>106829</th>\n      <td>2023</td>\n      <td>113</td>\n      <td>1403</td>\n      <td>74</td>\n      <td>A</td>\n      <td>0</td>\n      <td>29</td>\n      <td>47</td>\n      <td>7</td>\n      <td>12</td>\n      <td>9</td>\n      <td>14</td>\n      <td>5</td>\n      <td>31</td>\n      <td>11</td>\n      <td>16</td>\n      <td>4</td>\n      <td>1</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>106830</th>\n      <td>2023</td>\n      <td>113</td>\n      <td>1405</td>\n      <td>84</td>\n      <td>H</td>\n      <td>0</td>\n      <td>32</td>\n      <td>61</td>\n      <td>10</td>\n      <td>20</td>\n      <td>10</td>\n      <td>13</td>\n      <td>7</td>\n      <td>25</td>\n      <td>16</td>\n      <td>7</td>\n      <td>7</td>\n      <td>7</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>106831</th>\n      <td>2023</td>\n      <td>113</td>\n      <td>1429</td>\n      <td>65</td>\n      <td>A</td>\n      <td>0</td>\n      <td>21</td>\n      <td>51</td>\n      <td>7</td>\n      <td>27</td>\n      <td>16</td>\n      <td>22</td>\n      <td>7</td>\n      <td>27</td>\n      <td>11</td>\n      <td>12</td>\n      <td>7</td>\n      <td>3</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>106832</th>\n      <td>2023</td>\n      <td>113</td>\n      <td>1433</td>\n      <td>88</td>\n      <td>A</td>\n      <td>0</td>\n      <td>31</td>\n      <td>59</td>\n      <td>11</td>\n      <td>19</td>\n      <td>15</td>\n      <td>18</td>\n      <td>8</td>\n      <td>23</td>\n      <td>14</td>\n      <td>9</td>\n      <td>6</td>\n      <td>5</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>106833</th>\n      <td>2023</td>\n      <td>113</td>\n      <td>1437</td>\n      <td>64</td>\n      <td>A</td>\n      <td>0</td>\n      <td>25</td>\n      <td>56</td>\n      <td>6</td>\n      <td>19</td>\n      <td>8</td>\n      <td>9</td>\n      <td>3</td>\n      <td>21</td>\n      <td>13</td>\n      <td>7</td>\n      <td>7</td>\n      <td>1</td>\n      <td>16</td>\n    </tr>\n  </tbody>\n</table>\n<p>106834 rows × 19 columns</p>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"losers","metadata":{"cell_id":"5ce554e29702459aa4e67315442f49c5","source_hash":"d0caaa67","execution_start":1678496026527,"execution_millis":37,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"application/vnd.deepnote.dataframe.v3+json":{"column_count":19,"row_count":106834,"columns":[{"name":"Season","dtype":"int64"},{"name":"DayNum","dtype":"int64"},{"name":"LTeamID","dtype":"int64"},{"name":"LScore","dtype":"int64"},{"name":"WLoc","dtype":"object"},{"name":"NumOT","dtype":"int64"},{"name":"LFGM","dtype":"int64"},{"name":"LFGA","dtype":"int64"},{"name":"LFGM3","dtype":"int64"},{"name":"LFGA3","dtype":"int64"},{"name":"LFTM","dtype":"int64"},{"name":"LFTA","dtype":"int64"},{"name":"LOR","dtype":"int64"},{"name":"LDR","dtype":"int64"},{"name":"LAst","dtype":"int64"},{"name":"LTO","dtype":"int64"},{"name":"LStl","dtype":"int64"},{"name":"LBlk","dtype":"int64"},{"name":"LPF","dtype":"int64"},{"name":"_deepnote_index_column","dtype":"int64"}],"rows":[{"Season":"2003","DayNum":"10","LTeamID":"1328","LScore":"62","WLoc":"N","NumOT":"0","LFGM":"22","LFGA":"53","LFGM3":"2","LFGA3":"10","LFTM":"16","LFTA":"22","LOR":"10","LDR":"22","LAst":"8","LTO":"18","LStl":"9","LBlk":"2","LPF":"20","_deepnote_index_column":"0"},{"Season":"2003","DayNum":"10","LTeamID":"1393","LScore":"63","WLoc":"N","NumOT":"0","LFGM":"24","LFGA":"67","LFGM3":"6","LFGA3":"24","LFTM":"9","LFTA":"20","LOR":"20","LDR":"25","LAst":"7","LTO":"12","LStl":"8","LBlk":"6","LPF":"16","_deepnote_index_column":"1"},{"Season":"2003","DayNum":"11","LTeamID":"1437","LScore":"61","WLoc":"N","NumOT":"0","LFGM":"22","LFGA":"73","LFGM3":"3","LFGA3":"26","LFTM":"14","LFTA":"23","LOR":"31","LDR":"22","LAst":"9","LTO":"12","LStl":"2","LBlk":"5","LPF":"23","_deepnote_index_column":"2"},{"Season":"2003","DayNum":"11","LTeamID":"1457","LScore":"50","WLoc":"N","NumOT":"0","LFGM":"18","LFGA":"49","LFGM3":"6","LFGA3":"22","LFTM":"8","LFTA":"15","LOR":"17","LDR":"20","LAst":"9","LTO":"19","LStl":"4","LBlk":"3","LPF":"23","_deepnote_index_column":"3"},{"Season":"2003","DayNum":"11","LTeamID":"1208","LScore":"71","WLoc":"N","NumOT":"0","LFGM":"24","LFGA":"62","LFGM3":"6","LFGA3":"16","LFTM":"17","LFTA":"27","LOR":"21","LDR":"15","LAst":"12","LTO":"10","LStl":"7","LBlk":"1","LPF":"14","_deepnote_index_column":"4"},{"Season":"2003","DayNum":"11","LTeamID":"1186","LScore":"55","WLoc":"H","NumOT":"0","LFGM":"20","LFGA":"46","LFGM3":"3","LFGA3":"11","LFTM":"12","LFTA":"17","LOR":"6","LDR":"22","LAst":"8","LTO":"19","LStl":"4","LBlk":"3","LPF":"25","_deepnote_index_column":"5"},{"Season":"2003","DayNum":"12","LTeamID":"1236","LScore":"62","WLoc":"H","NumOT":"0","LFGM":"19","LFGA":"41","LFGM3":"4","LFGA3":"15","LFTM":"20","LFTA":"28","LOR":"9","LDR":"21","LAst":"11","LTO":"30","LStl":"10","LBlk":"4","LPF":"28","_deepnote_index_column":"6"},{"Season":"2003","DayNum":"12","LTeamID":"1457","LScore":"61","WLoc":"N","NumOT":"0","LFGM":"20","LFGA":"59","LFGM3":"4","LFGA3":"17","LFTM":"17","LFTA":"23","LOR":"8","LDR":"25","LAst":"10","LTO":"15","LStl":"14","LBlk":"8","LPF":"18","_deepnote_index_column":"7"},{"Season":"2003","DayNum":"12","LTeamID":"1156","LScore":"66","WLoc":"N","NumOT":"0","LFGM":"24","LFGA":"52","LFGM3":"6","LFGA3":"18","LFTM":"12","LFTA":"27","LOR":"13","LDR":"26","LAst":"13","LTO":"25","LStl":"8","LBlk":"2","LPF":"18","_deepnote_index_column":"8"},{"Season":"2003","DayNum":"12","LTeamID":"1296","LScore":"56","WLoc":"H","NumOT":"0","LFGM":"23","LFGA":"52","LFGM3":"3","LFGA3":"14","LFTM":"7","LFTA":"12","LOR":"9","LDR":"23","LAst":"10","LTO":"18","LStl":"1","LBlk":"3","LPF":"18","_deepnote_index_column":"9"}]},"text/plain":"        Season  DayNum  LTeamID  LScore WLoc  NumOT  LFGM  LFGA  LFGM3  LFGA3  \\\n0         2003      10     1328      62    N      0    22    53      2     10   \n1         2003      10     1393      63    N      0    24    67      6     24   \n2         2003      11     1437      61    N      0    22    73      3     26   \n3         2003      11     1457      50    N      0    18    49      6     22   \n4         2003      11     1208      71    N      0    24    62      6     16   \n...        ...     ...      ...     ...  ...    ...   ...   ...    ...    ...   \n106829    2023     113     1328      63    A      0    21    59      9     33   \n106830    2023     113     1103      63    H      0    26    59      4     21   \n106831    2023     113     1461      55    A      0    17    54      6     23   \n106832    2023     113     1386      63    A      0    21    56      8     28   \n106833    2023     113     1462      63    A      0    23    51      8     23   \n\n        LFTM  LFTA  LOR  LDR  LAst  LTO  LStl  LBlk  LPF  \n0         16    22   10   22     8   18     9     2   20  \n1          9    20   20   25     7   12     8     6   16  \n2         14    23   31   22     9   12     2     5   23  \n3          8    15   17   20     9   19     4     3   23  \n4         17    27   21   15    12   10     7     1   14  \n...      ...   ...  ...  ...   ...  ...   ...   ...  ...  \n106829    12    15    6   13    13    8    10     1   12  \n106830     7    14    9   23    10   10     3     0   11  \n106831    15    20    7   25     6   11     6     3   19  \n106832    13    18    5   20    10   11     3     3   15  \n106833     9    12    6   27    17   14     4     1   10  \n\n[106834 rows x 19 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Season</th>\n      <th>DayNum</th>\n      <th>LTeamID</th>\n      <th>LScore</th>\n      <th>WLoc</th>\n      <th>NumOT</th>\n      <th>LFGM</th>\n      <th>LFGA</th>\n      <th>LFGM3</th>\n      <th>LFGA3</th>\n      <th>LFTM</th>\n      <th>LFTA</th>\n      <th>LOR</th>\n      <th>LDR</th>\n      <th>LAst</th>\n      <th>LTO</th>\n      <th>LStl</th>\n      <th>LBlk</th>\n      <th>LPF</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2003</td>\n      <td>10</td>\n      <td>1328</td>\n      <td>62</td>\n      <td>N</td>\n      <td>0</td>\n      <td>22</td>\n      <td>53</td>\n      <td>2</td>\n      <td>10</td>\n      <td>16</td>\n      <td>22</td>\n      <td>10</td>\n      <td>22</td>\n      <td>8</td>\n      <td>18</td>\n      <td>9</td>\n      <td>2</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2003</td>\n      <td>10</td>\n      <td>1393</td>\n      <td>63</td>\n      <td>N</td>\n      <td>0</td>\n      <td>24</td>\n      <td>67</td>\n      <td>6</td>\n      <td>24</td>\n      <td>9</td>\n      <td>20</td>\n      <td>20</td>\n      <td>25</td>\n      <td>7</td>\n      <td>12</td>\n      <td>8</td>\n      <td>6</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2003</td>\n      <td>11</td>\n      <td>1437</td>\n      <td>61</td>\n      <td>N</td>\n      <td>0</td>\n      <td>22</td>\n      <td>73</td>\n      <td>3</td>\n      <td>26</td>\n      <td>14</td>\n      <td>23</td>\n      <td>31</td>\n      <td>22</td>\n      <td>9</td>\n      <td>12</td>\n      <td>2</td>\n      <td>5</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2003</td>\n      <td>11</td>\n      <td>1457</td>\n      <td>50</td>\n      <td>N</td>\n      <td>0</td>\n      <td>18</td>\n      <td>49</td>\n      <td>6</td>\n      <td>22</td>\n      <td>8</td>\n      <td>15</td>\n      <td>17</td>\n      <td>20</td>\n      <td>9</td>\n      <td>19</td>\n      <td>4</td>\n      <td>3</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2003</td>\n      <td>11</td>\n      <td>1208</td>\n      <td>71</td>\n      <td>N</td>\n      <td>0</td>\n      <td>24</td>\n      <td>62</td>\n      <td>6</td>\n      <td>16</td>\n      <td>17</td>\n      <td>27</td>\n      <td>21</td>\n      <td>15</td>\n      <td>12</td>\n      <td>10</td>\n      <td>7</td>\n      <td>1</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>106829</th>\n      <td>2023</td>\n      <td>113</td>\n      <td>1328</td>\n      <td>63</td>\n      <td>A</td>\n      <td>0</td>\n      <td>21</td>\n      <td>59</td>\n      <td>9</td>\n      <td>33</td>\n      <td>12</td>\n      <td>15</td>\n      <td>6</td>\n      <td>13</td>\n      <td>13</td>\n      <td>8</td>\n      <td>10</td>\n      <td>1</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>106830</th>\n      <td>2023</td>\n      <td>113</td>\n      <td>1103</td>\n      <td>63</td>\n      <td>H</td>\n      <td>0</td>\n      <td>26</td>\n      <td>59</td>\n      <td>4</td>\n      <td>21</td>\n      <td>7</td>\n      <td>14</td>\n      <td>9</td>\n      <td>23</td>\n      <td>10</td>\n      <td>10</td>\n      <td>3</td>\n      <td>0</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>106831</th>\n      <td>2023</td>\n      <td>113</td>\n      <td>1461</td>\n      <td>55</td>\n      <td>A</td>\n      <td>0</td>\n      <td>17</td>\n      <td>54</td>\n      <td>6</td>\n      <td>23</td>\n      <td>15</td>\n      <td>20</td>\n      <td>7</td>\n      <td>25</td>\n      <td>6</td>\n      <td>11</td>\n      <td>6</td>\n      <td>3</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>106832</th>\n      <td>2023</td>\n      <td>113</td>\n      <td>1386</td>\n      <td>63</td>\n      <td>A</td>\n      <td>0</td>\n      <td>21</td>\n      <td>56</td>\n      <td>8</td>\n      <td>28</td>\n      <td>13</td>\n      <td>18</td>\n      <td>5</td>\n      <td>20</td>\n      <td>10</td>\n      <td>11</td>\n      <td>3</td>\n      <td>3</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>106833</th>\n      <td>2023</td>\n      <td>113</td>\n      <td>1462</td>\n      <td>63</td>\n      <td>A</td>\n      <td>0</td>\n      <td>23</td>\n      <td>51</td>\n      <td>8</td>\n      <td>23</td>\n      <td>9</td>\n      <td>12</td>\n      <td>6</td>\n      <td>27</td>\n      <td>17</td>\n      <td>14</td>\n      <td>4</td>\n      <td>1</td>\n      <td>10</td>\n    </tr>\n  </tbody>\n</table>\n<p>106834 rows × 19 columns</p>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"Adding a \"Result\" column. If GREATER id wins, 1. If LOWER id wins, 0. ","metadata":{"tags":[],"cell_id":"b657a219e7444ff29b89c5994eb6d0f7","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"Also, re-organizing the table from winner's stats and loser's stats to higher/lower team ID's stats. ","metadata":{"tags":[],"cell_id":"5d96f4d586b14bfea846980ca5d241af","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"code","source":"df2 = df.copy(deep= True)\ndf2[\"Result\"] = np.select([df[\"WTeamID\"] > df[\"LTeamID\"], df[\"WTeamID\"] < df[\"LTeamID\"]], [1, 0])\nidx = (df2[\"Result\"] == 0)\ndf2.loc[idx,[\"WTeamID\", \"WScore\", \"LTeamID\", \"LScore\", \"WFGM\", \"WFGA\", \"WFGM3\", \"WFGA3\", \"WFTM\", \"WFTA\", \"WOR\", \"WDR\", \"WAst\", \"WTO\", \"WStl\", \"WBlk\", \"WPF\", \"LFGM\", \"LFGA\", \"LFGM3\", \"LFGA3\", \"LFTM\", \"LFTA\", \"LOR\", \"LDR\", \"LAst\", \"LTO\", \"LStl\", \"LBlk\", \"LPF\"]] = df2.loc[idx,[\"LTeamID\", \"LScore\", \"WTeamID\", \"WScore\", \"LFGM\", \"LFGA\", \"LFGM3\", \"LFGA3\", \"LFTM\", \"LFTA\", \"LOR\", \"LDR\", \"LAst\", \"LTO\", \"LStl\", \"LBlk\", \"LPF\", \"WFGM\", \"WFGA\", \"WFGM3\", \"WFGA3\", \"WFTM\", \"WFTA\", \"WOR\", \"WDR\", \"WAst\", \"WTO\", \"WStl\", \"WBlk\", \"WPF\"]].values\ndf2 = df2.rename(columns={\"WTeamID\":\"HTeamID\", \"WScore\":\"HScore\", \"WFGM\":\"HFGM\", \"WFGA\":\"HFGA\", \"WFGM3\":\"HFGM3\", \"WFGA3\":\"HFGA3\", \"WFTM\":\"HFTM\", \"WFTA\":\"HFTA\", \"WOR\":\"HOR\", \"WDR\":\"HDR\", \"WAst\":\"HAst\", \"WTO\":\"HTO\", \"WStl\":\"HStl\", \"WBlk\":\"HBlk\", \"WPF\":\"HPF\"})\ndf2","metadata":{"cell_id":"2db2ae054ff54465be972c8d0bc20515","source_hash":"c305e62d","execution_start":1678496026579,"execution_millis":638,"deepnote_table_state":{"sortBy":[],"filters":[],"pageSize":10,"pageIndex":10682},"deepnote_table_loading":false,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"application/vnd.deepnote.dataframe.v3+json":{"column_count":35,"row_count":106834,"columns":[{"name":"Season","dtype":"int64"},{"name":"DayNum","dtype":"int64"},{"name":"HTeamID","dtype":"int64"},{"name":"HScore","dtype":"int64"},{"name":"LTeamID","dtype":"int64"},{"name":"LScore","dtype":"int64"},{"name":"WLoc","dtype":"object"},{"name":"NumOT","dtype":"int64"},{"name":"HFGM","dtype":"int64"},{"name":"HFGA","dtype":"int64"},{"name":"HFGM3","dtype":"int64"},{"name":"HFGA3","dtype":"int64"},{"name":"HFTM","dtype":"int64"},{"name":"HFTA","dtype":"int64"},{"name":"HOR","dtype":"int64"},{"name":"HDR","dtype":"int64"},{"name":"HAst","dtype":"int64"},{"name":"HTO","dtype":"int64"},{"name":"HStl","dtype":"int64"},{"name":"HBlk","dtype":"int64"},{"name":"HPF","dtype":"int64"},{"name":"LFGM","dtype":"int64"},{"name":"LFGA","dtype":"int64"},{"name":"LFGM3","dtype":"int64"},{"name":"LFGA3","dtype":"int64"},{"name":"LFTM","dtype":"int64"},{"name":"LFTA","dtype":"int64"},{"name":"LOR","dtype":"int64"},{"name":"LDR","dtype":"int64"},{"name":"LAst","dtype":"int64"},{"name":"LTO","dtype":"int64"},{"name":"LStl","dtype":"int64"},{"name":"LBlk","dtype":"int64"},{"name":"LPF","dtype":"int64"},{"name":"Result","dtype":"int64"},{"name":"_deepnote_index_column","dtype":"int64"}],"rows":[{"Season":"2023","DayNum":"113","HTeamID":"1293","HScore":"69","LTeamID":"1283","LScore":"84","WLoc":"H","NumOT":"0","HFGM":"22","HFGA":"53","HFGM3":"12","HFGA3":"23","HFTM":"13","HFTA":"16","HOR":"5","HDR":"19","HAst":"11","HTO":"11","HStl":"5","HBlk":"2","HPF":"9","LFGM":"33","LFGA":"64","LFGM3":"13","LFGA3":"28","LFTM":"5","LFTA":"7","LOR":"10","LDR":"22","LAst":"20","LTO":"7","LStl":"6","LBlk":"6","LPF":"14","Result":"0","_deepnote_index_column":"106820"},{"Season":"2023","DayNum":"113","HTeamID":"1363","HScore":"51","LTeamID":"1305","LScore":"66","WLoc":"H","NumOT":"0","HFGM":"20","HFGA":"55","HFGM3":"10","HFGA3":"33","HFTM":"1","HFTA":"2","HOR":"9","HDR":"22","HAst":"15","HTO":"7","HStl":"2","HBlk":"6","HPF":"17","LFGM":"24","LFGA":"52","LFGM3":"6","LFGA3":"16","LFTM":"12","LFTA":"15","LOR":"4","LDR":"26","LAst":"15","LTO":"5","LStl":"4","LBlk":"3","LPF":"11","Result":"0","_deepnote_index_column":"106821"},{"Season":"2023","DayNum":"113","HTeamID":"1325","HScore":"77","LTeamID":"1296","LScore":"68","WLoc":"H","NumOT":"0","HFGM":"29","HFGA":"68","HFGM3":"9","HFGA3":"28","HFTM":"10","HFTA":"14","HOR":"13","HDR":"25","HAst":"14","HTO":"10","HStl":"6","HBlk":"4","HPF":"15","LFGM":"29","LFGA":"62","LFGM3":"1","LFGA3":"19","LFTM":"9","LFTA":"19","LOR":"10","LDR":"23","LAst":"13","LTO":"11","LStl":"7","LBlk":"1","LPF":"15","Result":"1","_deepnote_index_column":"106822"},{"Season":"2023","DayNum":"113","HTeamID":"1338","HScore":"76","LTeamID":"1210","LScore":"68","WLoc":"H","NumOT":"0","HFGM":"24","HFGA":"51","HFGM3":"9","HFGA3":"24","HFTM":"19","HFTA":"20","HOR":"4","HDR":"26","HAst":"14","HTO":"6","HStl":"1","HBlk":"2","HPF":"12","LFGM":"26","LFGA":"55","LFGM3":"9","LFGA3":"28","LFTM":"7","LFTA":"10","LOR":"3","LDR":"21","LAst":"13","LTO":"3","LStl":"0","LBlk":"3","LPF":"17","Result":"1","_deepnote_index_column":"106823"},{"Season":"2023","DayNum":"113","HTeamID":"1387","HScore":"78","LTeamID":"1350","LScore":"81","WLoc":"H","NumOT":"0","HFGM":"32","HFGA":"60","HFGM3":"8","HFGA3":"20","HFTM":"6","HFTA":"7","HOR":"7","HDR":"26","HAst":"22","HTO":"12","HStl":"6","HBlk":"1","HPF":"18","LFGM":"24","LFGA":"54","LFGM3":"17","LFGA3":"37","LFTM":"16","LFTA":"20","LOR":"7","LDR":"21","LAst":"15","LTO":"9","LStl":"5","LBlk":"0","LPF":"10","Result":"0","_deepnote_index_column":"106824"},{"Season":"2023","DayNum":"113","HTeamID":"1464","HScore":"64","LTeamID":"1352","LScore":"83","WLoc":"H","NumOT":"0","HFGM":"23","HFGA":"47","HFGM3":"4","HFGA3":"12","HFTM":"14","HFTA":"20","HOR":"4","HDR":"16","HAst":"10","HTO":"16","HStl":"6","HBlk":"0","HPF":"14","LFGM":"34","LFGA":"54","LFGM3":"8","LFGA3":"16","LFTM":"7","LFTA":"9","LOR":"4","LDR":"20","LAst":"14","LTO":"12","LStl":"6","LBlk":"7","LPF":"17","Result":"0","_deepnote_index_column":"106825"},{"Season":"2023","DayNum":"113","HTeamID":"1361","HScore":"77","LTeamID":"1161","LScore":"58","WLoc":"H","NumOT":"0","HFGM":"26","HFGA":"54","HFGM3":"6","HFGA3":"21","HFTM":"19","HFTA":"23","HOR":"10","HDR":"19","HAst":"13","HTO":"7","HStl":"9","HBlk":"3","HPF":"8","LFGM":"24","LFGA":"50","LFGM3":"3","LFGA3":"14","LFTM":"7","LFTA":"9","LOR":"7","LDR":"17","LAst":"9","LTO":"13","LStl":"5","LBlk":"1","LPF":"18","Result":"1","_deepnote_index_column":"106826"},{"Season":"2023","DayNum":"113","HTeamID":"1400","HScore":"72","LTeamID":"1235","LScore":"54","WLoc":"H","NumOT":"0","HFGM":"27","HFGA":"57","HFGM3":"12","HFGA3":"28","HFTM":"6","HFTA":"11","HOR":"11","HDR":"17","HAst":"18","HTO":"11","HStl":"9","HBlk":"1","HPF":"16","LFGM":"20","LFGA":"47","LFGM3":"4","LFGA3":"19","LFTM":"10","LFTA":"13","LOR":"7","LDR":"17","LAst":"8","LTO":"15","LStl":"6","LBlk":"1","LPF":"14","Result":"1","_deepnote_index_column":"106827"},{"Season":"2023","DayNum":"113","HTeamID":"1401","HScore":"68","LTeamID":"1397","LScore":"63","WLoc":"H","NumOT":"0","HFGM":"18","HFGA":"46","HFGM3":"4","HFGA3":"15","HFTM":"28","HFTA":"34","HOR":"11","HDR":"19","HAst":"9","HTO":"14","HStl":"9","HBlk":"1","HPF":"18","LFGM":"22","LFGA":"55","LFGM3":"9","LFGA3":"31","LFTM":"10","LFTA":"14","LOR":"12","LDR":"18","LAst":"19","LTO":"13","LStl":"9","LBlk":"4","LPF":"24","Result":"1","_deepnote_index_column":"106828"},{"Season":"2023","DayNum":"113","HTeamID":"1403","HScore":"74","LTeamID":"1328","LScore":"63","WLoc":"A","NumOT":"0","HFGM":"29","HFGA":"47","HFGM3":"7","HFGA3":"12","HFTM":"9","HFTA":"14","HOR":"5","HDR":"31","HAst":"11","HTO":"16","HStl":"4","HBlk":"1","HPF":"17","LFGM":"21","LFGA":"59","LFGM3":"9","LFGA3":"33","LFTM":"12","LFTA":"15","LOR":"6","LDR":"13","LAst":"13","LTO":"8","LStl":"10","LBlk":"1","LPF":"12","Result":"1","_deepnote_index_column":"106829"}]},"text/plain":"        Season  DayNum  HTeamID  HScore  LTeamID  LScore WLoc  NumOT  HFGM  \\\n0         2003      10     1328      62     1104      68    N      0    22   \n1         2003      10     1393      63     1272      70    N      0    24   \n2         2003      11     1437      61     1266      73    N      0    22   \n3         2003      11     1457      50     1296      56    N      0    18   \n4         2003      11     1400      77     1208      71    N      0    30   \n...        ...     ...      ...     ...      ...     ...  ...    ...   ...   \n106829    2023     113     1403      74     1328      63    A      0    29   \n106830    2023     113     1405      84     1103      63    H      0    32   \n106831    2023     113     1461      55     1429      65    A      0    17   \n106832    2023     113     1433      88     1386      63    A      0    31   \n106833    2023     113     1462      63     1437      64    A      0    23   \n\n        HFGA  ...  LFTM  LFTA  LOR  LDR  LAst  LTO  LStl  LBlk  LPF  Result  \n0         53  ...    11    18   14   24    13   23     7     1   22       0  \n1         67  ...    10    19   15   28    16   13     4     4   18       0  \n2         73  ...    17    29   17   26    15   10     5     2   25       0  \n3         49  ...    17    31    6   19    11   12    14     2   18       0  \n4         61  ...    17    27   21   15    12   10     7     1   14       1  \n...      ...  ...   ...   ...  ...  ...   ...  ...   ...   ...  ...     ...  \n106829    47  ...    12    15    6   13    13    8    10     1   12       1  \n106830    61  ...     7    14    9   23    10   10     3     0   11       1  \n106831    54  ...    16    22    7   27    11   12     7     3   15       0  \n106832    59  ...    13    18    5   20    10   11     3     3   15       1  \n106833    51  ...     8     9    3   21    13    7     7     1   16       0  \n\n[106834 rows x 35 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Season</th>\n      <th>DayNum</th>\n      <th>HTeamID</th>\n      <th>HScore</th>\n      <th>LTeamID</th>\n      <th>LScore</th>\n      <th>WLoc</th>\n      <th>NumOT</th>\n      <th>HFGM</th>\n      <th>HFGA</th>\n      <th>...</th>\n      <th>LFTM</th>\n      <th>LFTA</th>\n      <th>LOR</th>\n      <th>LDR</th>\n      <th>LAst</th>\n      <th>LTO</th>\n      <th>LStl</th>\n      <th>LBlk</th>\n      <th>LPF</th>\n      <th>Result</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2003</td>\n      <td>10</td>\n      <td>1328</td>\n      <td>62</td>\n      <td>1104</td>\n      <td>68</td>\n      <td>N</td>\n      <td>0</td>\n      <td>22</td>\n      <td>53</td>\n      <td>...</td>\n      <td>11</td>\n      <td>18</td>\n      <td>14</td>\n      <td>24</td>\n      <td>13</td>\n      <td>23</td>\n      <td>7</td>\n      <td>1</td>\n      <td>22</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2003</td>\n      <td>10</td>\n      <td>1393</td>\n      <td>63</td>\n      <td>1272</td>\n      <td>70</td>\n      <td>N</td>\n      <td>0</td>\n      <td>24</td>\n      <td>67</td>\n      <td>...</td>\n      <td>10</td>\n      <td>19</td>\n      <td>15</td>\n      <td>28</td>\n      <td>16</td>\n      <td>13</td>\n      <td>4</td>\n      <td>4</td>\n      <td>18</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2003</td>\n      <td>11</td>\n      <td>1437</td>\n      <td>61</td>\n      <td>1266</td>\n      <td>73</td>\n      <td>N</td>\n      <td>0</td>\n      <td>22</td>\n      <td>73</td>\n      <td>...</td>\n      <td>17</td>\n      <td>29</td>\n      <td>17</td>\n      <td>26</td>\n      <td>15</td>\n      <td>10</td>\n      <td>5</td>\n      <td>2</td>\n      <td>25</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2003</td>\n      <td>11</td>\n      <td>1457</td>\n      <td>50</td>\n      <td>1296</td>\n      <td>56</td>\n      <td>N</td>\n      <td>0</td>\n      <td>18</td>\n      <td>49</td>\n      <td>...</td>\n      <td>17</td>\n      <td>31</td>\n      <td>6</td>\n      <td>19</td>\n      <td>11</td>\n      <td>12</td>\n      <td>14</td>\n      <td>2</td>\n      <td>18</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2003</td>\n      <td>11</td>\n      <td>1400</td>\n      <td>77</td>\n      <td>1208</td>\n      <td>71</td>\n      <td>N</td>\n      <td>0</td>\n      <td>30</td>\n      <td>61</td>\n      <td>...</td>\n      <td>17</td>\n      <td>27</td>\n      <td>21</td>\n      <td>15</td>\n      <td>12</td>\n      <td>10</td>\n      <td>7</td>\n      <td>1</td>\n      <td>14</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>106829</th>\n      <td>2023</td>\n      <td>113</td>\n      <td>1403</td>\n      <td>74</td>\n      <td>1328</td>\n      <td>63</td>\n      <td>A</td>\n      <td>0</td>\n      <td>29</td>\n      <td>47</td>\n      <td>...</td>\n      <td>12</td>\n      <td>15</td>\n      <td>6</td>\n      <td>13</td>\n      <td>13</td>\n      <td>8</td>\n      <td>10</td>\n      <td>1</td>\n      <td>12</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>106830</th>\n      <td>2023</td>\n      <td>113</td>\n      <td>1405</td>\n      <td>84</td>\n      <td>1103</td>\n      <td>63</td>\n      <td>H</td>\n      <td>0</td>\n      <td>32</td>\n      <td>61</td>\n      <td>...</td>\n      <td>7</td>\n      <td>14</td>\n      <td>9</td>\n      <td>23</td>\n      <td>10</td>\n      <td>10</td>\n      <td>3</td>\n      <td>0</td>\n      <td>11</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>106831</th>\n      <td>2023</td>\n      <td>113</td>\n      <td>1461</td>\n      <td>55</td>\n      <td>1429</td>\n      <td>65</td>\n      <td>A</td>\n      <td>0</td>\n      <td>17</td>\n      <td>54</td>\n      <td>...</td>\n      <td>16</td>\n      <td>22</td>\n      <td>7</td>\n      <td>27</td>\n      <td>11</td>\n      <td>12</td>\n      <td>7</td>\n      <td>3</td>\n      <td>15</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>106832</th>\n      <td>2023</td>\n      <td>113</td>\n      <td>1433</td>\n      <td>88</td>\n      <td>1386</td>\n      <td>63</td>\n      <td>A</td>\n      <td>0</td>\n      <td>31</td>\n      <td>59</td>\n      <td>...</td>\n      <td>13</td>\n      <td>18</td>\n      <td>5</td>\n      <td>20</td>\n      <td>10</td>\n      <td>11</td>\n      <td>3</td>\n      <td>3</td>\n      <td>15</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>106833</th>\n      <td>2023</td>\n      <td>113</td>\n      <td>1462</td>\n      <td>63</td>\n      <td>1437</td>\n      <td>64</td>\n      <td>A</td>\n      <td>0</td>\n      <td>23</td>\n      <td>51</td>\n      <td>...</td>\n      <td>8</td>\n      <td>9</td>\n      <td>3</td>\n      <td>21</td>\n      <td>13</td>\n      <td>7</td>\n      <td>7</td>\n      <td>1</td>\n      <td>16</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>106834 rows × 35 columns</p>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"from tensorflow import keras\nfrom tensorflow.keras.datasets import boston_housing\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation","metadata":{"tags":[],"cell_id":"4f70f98b26ef48a2b14aaa26b391f7e3","source_hash":"5e8c463d","execution_start":1678496027219,"execution_millis":5,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"train data: data used for training the model","metadata":{"tags":[],"cell_id":"670676d263ea4ccb8a9389fdadda58af","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"test data: data used for testing the model","metadata":{"tags":[],"cell_id":"fb87a09d16a849c0bcd0788121445203","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"code","source":"train_df = df2.sample(frac = 0.8, random_state = 0)\ntest_df = df2.drop(train_df.index)\n\nx_train = train_df.iloc[:, np.r_[2:5, 7:33]]\ny_train = train_df[[\"Result\"]]\n\nx_test = test_df.iloc[:, np.r_[2:5, 7:33]]\ny_test = test_df[[\"Result\"]] \ny_test\n","metadata":{"tags":[],"cell_id":"c54ac08b85d14286afc26107842559a2","source_hash":"57b82014","execution_start":1678496027268,"execution_millis":59,"deepnote_table_state":{"sortBy":[],"filters":[],"pageSize":10,"pageIndex":0},"deepnote_table_loading":false,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"application/vnd.deepnote.dataframe.v3+json":{"column_count":1,"row_count":21367,"columns":[{"name":"Result","dtype":"int64","stats":{"unique_count":2,"nan_count":0,"min":"0","max":"1","histogram":[{"bin_start":0,"bin_end":0.1,"count":10410},{"bin_start":0.1,"bin_end":0.2,"count":0},{"bin_start":0.2,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0},{"bin_start":0.5,"bin_end":0.6000000000000001,"count":0},{"bin_start":0.6000000000000001,"bin_end":0.7000000000000001,"count":0},{"bin_start":0.7000000000000001,"bin_end":0.8,"count":0},{"bin_start":0.8,"bin_end":0.9,"count":0},{"bin_start":0.9,"bin_end":1,"count":10957}]}},{"name":"_deepnote_index_column","dtype":"int64"}],"rows":[{"Result":"0","_deepnote_index_column":"2"},{"Result":"0","_deepnote_index_column":"10"},{"Result":"1","_deepnote_index_column":"13"},{"Result":"1","_deepnote_index_column":"20"},{"Result":"1","_deepnote_index_column":"21"},{"Result":"0","_deepnote_index_column":"27"},{"Result":"0","_deepnote_index_column":"30"},{"Result":"0","_deepnote_index_column":"43"},{"Result":"0","_deepnote_index_column":"46"},{"Result":"0","_deepnote_index_column":"55"}]},"text/plain":"        Result\n2            0\n10           0\n13           1\n20           1\n21           1\n...        ...\n106796       1\n106798       0\n106805       1\n106809       0\n106813       1\n\n[21367 rows x 1 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Result</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>106796</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>106798</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>106805</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>106809</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>106813</th>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>21367 rows × 1 columns</p>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"Sequential Model: Straight line of \"Tensors\", with arbitrary number of \"Layers\". We use 2 \"Dense\" layers. Each layer predicts increasingly abstract data. Input dimension specifies size of inputted array. Number of neurons in the final layer specifies size of outputted array. ","metadata":{"tags":[],"cell_id":"15be5f4fa9b44b01b8d754ce910e1155","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"- Soooooo many different types of layers. Can also create custom layers (super fucked up)","metadata":{"tags":[],"cell_id":"cf44a202-fbaa-40a1-97ea-f6d03dbef120","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-bullet"}},{"cell_type":"markdown","source":"- Number of layers corresponds to complexity of the task at hand - you would want to use many layers to deal with abstract problems such as image/speech recognition. More layers allows the model to find more complex representations of data, which would improve accuracy. Using too many can lead to, you guessed it, overfitting (as usual). Use less layers for simpler tasks such as binary classification or regression (literally what we're doing right now).","metadata":{"tags":[],"cell_id":"8853de1a-4b10-4f1c-a61f-2220f4f0e266","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":63,"fromCodePoint":32}],"deepnote_cell_type":"text-cell-bullet"}},{"cell_type":"markdown","source":"- Number of neurons in each layer corresponds to size of training dataset - allows model to notice more complex representations of data in a different way (I would assume). May need to increase regularization to prevent overfitting (I bolded it bc we probably need to do that)","metadata":{"tags":[],"cell_id":"2bfdbbb1-e85c-43b1-919b-9a5a2af9f200","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":72,"fromCodePoint":47},{"type":"marks","marks":{"bold":true},"toCodePoint":229,"fromCodePoint":183}],"deepnote_cell_type":"text-cell-bullet"}},{"cell_type":"markdown","source":"Dense Layer: Every neuron connected to a neuron in the previous layer. Output of each neuron is a weighted sum of the inputs plus a \"bias\" term. ","metadata":{"tags":[],"cell_id":"2375c960143d486bb56b769f95007c13","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":11,"fromCodePoint":0}],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"- Dense layer is how we would traditionally think of a weighted sum in linear algebra. The input array is multiplied by a \"kernel\" array, made up of the weights that the model constantly adjusts with each epoch. A \"bias\" array is added to the resulting dot product, also adjusted by the model with each epoch. ","metadata":{"tags":[],"cell_id":"12bb2bdb-59c3-4029-a4d7-429363c33de4","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-bullet"}},{"cell_type":"markdown","source":"General summary of the parameters in each layer:","metadata":{"tags":[],"cell_id":"79a3fa0f-0b8b-4fd8-b872-d86bf0c8eca6","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"- First parameter: number of neurons in the layer. Controls complexity and capacity, if too large then overfit risk","metadata":{"tags":[],"cell_id":"332c935eb36b49969f54903bc165e1fb","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":16,"fromCodePoint":0}],"deepnote_cell_type":"text-cell-bullet"}},{"cell_type":"markdown","source":"- Second parameter: the activation function to use. Determines the output of a neuron given the weighted sum of the inputs plus a bias term. ","metadata":{"tags":[],"cell_id":"db5a288a7ec74da7b3a4fb3f1b4e891d","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":17,"fromCodePoint":0}],"deepnote_cell_type":"text-cell-bullet"}},{"cell_type":"markdown","source":"- \"relu\" - rectified linear unit activation function, returns element-wise maximum of 0 and the input tensor. Now realizing this may  not be a great option of the input array has negative values. There are alternatives, but using this with negative numbers would result in a \"dying ReLU problem\"","metadata":{"tags":[],"cell_id":"09282a14-694a-4a67-80cf-cc13d8df0801","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-bullet"}},{"cell_type":"markdown","source":"- \"sigmoid\" - maps inputs to the range 0 and 1. This is why we use sigmoid in the final layer: to map our output to a one-dimensional array with one value between 0 and 1, which abstractly represents the percent confidence that the team with the higher id will win. ","metadata":{"tags":[],"cell_id":"2e9edebc-f3dd-4664-a104-4d989b101c44","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-bullet"}},{"cell_type":"markdown","source":"- Another popular one that we're not using (yet) is \"softmax\" - From what I understand, does the same as sigmoid except every element in the resulting matrix will sum to one. Popular when dealing with a probability distribution problem, which you could view our problem as. From and abstract perspective, our final dense layer of one neuron with a \"sigmoid\" versus a \"softmax\" activation SHOULD perform the exact same task. I chose sigmoid because chatgpt knows all and sees all. ","metadata":{"tags":[],"cell_id":"e21e7468-0b29-4780-9864-129fb860c91c","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-bullet"}},{"cell_type":"markdown","source":"- Third parameter: number of input features. Only needed for the first layer, the rest can infer the input_dim based on the previous layer. On the FINAL layer, the number of neurons corresponds to the dimension of the output","metadata":{"tags":[],"cell_id":"7b8510398421452588f1a191b24d82df","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":16,"fromCodePoint":0}],"deepnote_cell_type":"text-cell-bullet"}},{"cell_type":"markdown","source":"Idea: make the final layer w/ 2 neurons and softmax activation. Can have one neuron output result if higher team id wins and other output result of lower team id winning. Would have to add a column to the input, basically one column for the higher team id winning and other for lower team id winning. Then we could take the maximum of the two outputs, which would be the probability of a given team winning. Alternatively, can do 1 minus the probability if the probability is less than 0.5, and make sure that we adjust it to the % chance that the lower team id wins rather than the higher one. Food for thought. ","metadata":{"tags":[],"cell_id":"b35a743f-7031-4f29-9334-fc751f04ebe6","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"Also this is a BINARY CLASSIFICATION PROBLEM, NOT A REGRESSION PROBLEM","metadata":{"tags":[],"cell_id":"91b38985-453b-45ee-a4a4-1dce58741877","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"code","source":"model = Sequential([\n    Dense(64, activation=\"relu\", input_dim=29),\n    Dense(32, activation='relu'),\n    Dense(1, activation= 'sigmoid')\n])","metadata":{"cell_id":"7d8a35e57df8417b9bc5f0b1abb79c39","source_hash":"3828c49f","execution_start":1678496027361,"execution_millis":64,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stderr","text":"2023-03-11 00:53:47.330924: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n2023-03-11 00:53:47.330958: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n2023-03-11 00:53:47.330977: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (p-76c1b032-5d0a-4d37-bf01-322f3e813bd4): /proc/driver/nvidia/version does not exist\n2023-03-11 00:53:47.331206: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"For compile: a million parameters we CAN change if we want. Basically telling the model what we want it to be good at doing. ","metadata":{"tags":[],"cell_id":"b1d76dd585b3428fbb494672262aa558","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"- Loss is the function that calculates the difference between the actual output and the expected output (trying to minimize this function). Use mean squared error for regression problems, categorical cross-entropy for classification problems. This is how we measure the effectiveness of the model in real-time, as the epochs iterate through the training data (watch the console).","metadata":{"tags":[],"cell_id":"63b44bd57fc54caf831ba0331fb65505","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":4,"fromCodePoint":0},{"type":"marks","marks":{"bold":true},"toCodePoint":377,"fromCodePoint":241}],"deepnote_cell_type":"text-cell-bullet"}},{"cell_type":"markdown","source":"- Metrics is an array of ways to evaluate the effectiveness of the model based on a training data set. NOT USED DURING TRAINING PROCESS. Use accuracy for classification problems, mean squared error for regression problems. ","metadata":{"tags":[],"cell_id":"27736d1b2bb14d7099d950471fe9306c","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":7,"fromCodePoint":0}],"deepnote_cell_type":"text-cell-bullet"}},{"cell_type":"markdown","source":"- Optimizer basically tells model HOW we want it to optimize (depends on nature of problem being solved and size of training data). Tbh I'm not even close to smart enough to understand wtf this shit is. I looked up the \"adam\" optimizer and discovered the following lovely sentence: \"Adam optimization is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments\". I'd rather jump off a bridge than figure out what that shit means. ","metadata":{"tags":[],"cell_id":"a25eadb2e7504e088a455a5c731fe134","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":9,"fromCodePoint":0}],"deepnote_cell_type":"text-cell-bullet"}},{"cell_type":"code","source":"model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['accuracy'])","metadata":{"tags":[],"cell_id":"e8f3bc50509142dfb616e74b72e00796","source_hash":"49c0e2bc","execution_start":1678496027430,"execution_millis":11,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"Fit is where we actually train the model with x and y train. Epochs = number of times the model iterates over the training data. ","metadata":{"tags":[],"cell_id":"76099b746be148269c5adac760060462","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"General theory for the number of epochs to use: ","metadata":{"tags":[],"cell_id":"c7e597c5-9588-464c-8c90-3af83f7725ee","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":39,"fromCodePoint":23}],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"- Early stopping: Used to prevent overfitting. Monitor model performance, set epoch number to before loss starts to increase again, which is a marker of overfitting. Helps prevent the model learning training data too closely, leads to poor generalization performance","metadata":{"tags":[],"cell_id":"f81f35c5-8b10-4ba8-8b9f-3bc8eddbf7b0","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":15,"fromCodePoint":0},{"type":"marks","marks":{"bold":true},"toCodePoint":128,"fromCodePoint":92}],"deepnote_cell_type":"text-cell-bullet"}},{"cell_type":"markdown","source":"- Complexity of the task: More complex = more epochs to converge on a good solution (optimal kernel matrix I think, see earlier explanation of dense layers). Model may need more time to learn the underlying patterns in the data.","metadata":{"tags":[],"cell_id":"6cf181d1-6516-4925-886c-eebe3a262f80","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":23,"fromCodePoint":0}],"deepnote_cell_type":"text-cell-bullet"}},{"cell_type":"markdown","source":"- Size of the dataset: Larger datasets = more epochs to learn the underlying patterns in the data, smaller datasets may converge more quickly.","metadata":{"tags":[],"cell_id":"afe7ace7-2c41-4e42-bc38-86d69704060d","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":20,"fromCodePoint":0}],"deepnote_cell_type":"text-cell-bullet"}},{"cell_type":"markdown","source":"- Architecture of the model: Different architectures may require different numbers of epochs to converge on a good solution. For example, deeper architectures may require more epochs to learn more complex representations of the data. << That was the chatgpt explanation, I honestly don't fully understand it or what architecture means yet. I'm assuming it's related to the complexity of the task, but some distinction has to exist (probably. Hopefully.)","metadata":{"tags":[],"cell_id":"e41971a9-c8a4-43fc-ac19-d5c492631331","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":26,"fromCodePoint":0}],"deepnote_cell_type":"text-cell-bullet"}},{"cell_type":"markdown","source":"- Training performance: Closely related to theory of early stopping, but on the other end: Adjust epochs by training performance. If loss continues to decrease, then more epochs (generally). May need more iterations to converge on optimal solution. Also, monitor the test data loss, which can be found using the .evaluate() method of the model object. Need to make sure that training and test losses both decrease together. If training loss decreases and test loss increases with the addition of an epoch(s), then overfitting has begun and all the model will be good for is memorizing the training data rather than applying generalizations to new data. ","metadata":{"tags":[],"cell_id":"2592e750-04a6-477c-8d47-4f48804b9086","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":21,"fromCodePoint":0},{"type":"marks","marks":{"bold":true},"toCodePoint":329,"fromCodePoint":310}],"deepnote_cell_type":"text-cell-bullet"}},{"cell_type":"markdown","source":"General theory for batch size to use:","metadata":{"tags":[],"cell_id":"b616aa13-cdd4-428e-8a1c-9b1a3a21dec8","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":30,"fromCodePoint":19}],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"Trade-off between accuracy and computational efficiency: A larger batch size can increase the accuracy of the gradients used to update the model's weights, but also requires more memory and computational resources. Conversely, a smaller batch size can be more computationally efficient but may result in less accuracy. ","metadata":{"tags":[],"cell_id":"8772547d-8cf7-4ef6-b861-7f3aeba55346","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":55,"fromCodePoint":0}],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"Size of the dataset: Larger datasets = larger batch size, goal is that each batch size is representative of the data as a whole. Smaller data sets may use smaller batch sizes. ","metadata":{"tags":[],"cell_id":"164b0a43-b401-41f4-8505-bfb89074d550","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":20,"fromCodePoint":0}],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"Memory constraints: batch size must be small enough for one batch to completely fit in the memory of the device being used","metadata":{"tags":[],"cell_id":"d7dfb9d6-a513-4174-a9f0-a6ecef6a2c24","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":20,"fromCodePoint":0}],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"Overfitting: Smaller batch sizes prevent overfitting by introducing more randomness in model training. However, may require more epochs to converge on accurate solution. ","metadata":{"tags":[],"cell_id":"cf4003ac-5051-47a7-8ea5-49f0aa98401b","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":13,"fromCodePoint":0}],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"Convergence rate: Larger batch sizes = faster convergence in some cases, depends on the specific problem and model architecture.","metadata":{"tags":[],"cell_id":"c785a269-1a61-4286-b026-f0cdcb958290","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":17,"fromCodePoint":0}],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"code","source":"model.fit(x_train, y_train, epochs=15, batch_size=32, validation_data=(x_test, y_test))","metadata":{"tags":[],"cell_id":"a09c0f7071b140bc97d5eaf327c31b9a","source_hash":"d2edaa0c","execution_start":1678496027447,"execution_millis":73402,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Epoch 1/15\n2671/2671 [==============================] - 9s 3ms/step - loss: 0.3225 - accuracy: 0.8834 - val_loss: 0.0952 - val_accuracy: 0.9606\nEpoch 2/15\n2671/2671 [==============================] - 8s 3ms/step - loss: 0.1552 - accuracy: 0.9388 - val_loss: 0.0744 - val_accuracy: 0.9684\nEpoch 3/15\n2671/2671 [==============================] - 8s 3ms/step - loss: 0.1442 - accuracy: 0.9457 - val_loss: 0.1051 - val_accuracy: 0.9540\nEpoch 4/15\n2671/2671 [==============================] - 8s 3ms/step - loss: 0.1139 - accuracy: 0.9577 - val_loss: 0.0437 - val_accuracy: 0.9831\nEpoch 5/15\n2671/2671 [==============================] - 8s 3ms/step - loss: 0.0888 - accuracy: 0.9651 - val_loss: 0.0564 - val_accuracy: 0.9754\nEpoch 6/15\n2671/2671 [==============================] - 8s 3ms/step - loss: 0.0773 - accuracy: 0.9694 - val_loss: 0.0896 - val_accuracy: 0.9591\nEpoch 7/15\n2671/2671 [==============================] - 8s 3ms/step - loss: 0.0671 - accuracy: 0.9728 - val_loss: 0.0493 - val_accuracy: 0.9766\nEpoch 8/15\n2671/2671 [==============================] - 8s 3ms/step - loss: 0.0619 - accuracy: 0.9746 - val_loss: 0.0305 - val_accuracy: 0.9901\nEpoch 9/15\n1815/2671 [===================>..........] - ETA: 2s - loss: 0.0615 - accuracy: 0.9760","output_type":"stream"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn [11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/keras/engine/training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1562\u001b[0m ):\n\u001b[1;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n","File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n","File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n","File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/tensorflow/python/eager/function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2494\u001b[0m   (graph_function,\n\u001b[1;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1865\u001b[0m     args,\n\u001b[1;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1867\u001b[0m     executing_eagerly)\n\u001b[1;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n","File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n","File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"execution_count":11},{"cell_type":"markdown","source":"Testing the model on the test data","metadata":{"tags":[],"cell_id":"aa461a638eb448f6aee6a2e38ddd73ec","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"code","source":"y_pred = model.predict(x_test)\n\npd.DataFrame(y_pred, columns = [\"Predictions\"])","metadata":{"tags":[],"cell_id":"ab705d7b79c340179190697bfe589453","source_hash":"8fce8f2f","execution_start":1678337594703,"execution_millis":158506149,"deepnote_table_state":{"sortBy":[],"filters":[],"pageSize":10,"pageIndex":32},"deepnote_table_loading":false,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":"Ideas for additional data:","metadata":{"tags":[],"cell_id":"d0f93cbca81d4a66bb8508539af76455","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"- Postseason AP poll","metadata":{"tags":[],"cell_id":"c4c63e50-cd79-40d7-ae5f-1d91ec7e9b69","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-bullet"}},{"cell_type":"markdown","source":"- Maybe only tourney games rather than reg season","metadata":{"tags":[],"cell_id":"00e0d42c-b7cd-4add-9e38-4074cb0d3574","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-bullet"}},{"cell_type":"markdown","source":"- Power 5 indicator","metadata":{"tags":[],"cell_id":"ca08cd02-1233-4503-8e3a-e9b3fe0c94fd","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-bullet"}},{"cell_type":"code","source":"x_train","metadata":{"tags":[],"cell_id":"6699c8913874408a856330dc569c942d","source_hash":"e396a278","execution_start":1678496007474,"execution_millis":93378,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":0},{"cell_type":"code","source":"","metadata":{"tags":[],"cell_id":"2adc5339333e41a28bd22f8af30d755e","source_hash":"b623e53d","deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=76c1b032-5d0a-4d37-bf01-322f3e813bd4' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"tags":[],"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"vscode":{"interpreter":{"hash":"0d2300c69255e45d05b784d6bea2adbb549887531e21e7df92e5f7b28d7f9454"}},"deepnote":{},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"language_info":{"name":"python","version":"3.9.13","mimetype":"text/x-python","file_extension":".py","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python"},"orig_nbformat":4,"deepnote_full_width":false,"deepnote_notebook_id":"6a91b1bceb6345d9b34fe3d2995e888e","deepnote_execution_queue":[]}}